# 面经

# 米哈游1面

1. 自我介绍，为什么想做大模型方向?
2. 拷打项目和实习
3. reward bench上的reward model分哪几类?reward model如何训练的，训练目标是什么?
4. dpo训练的损失函数和训练目标，dpo如何改进
5. 指令跟随能力的评估集有什么，如何评估的?
6. gsm8k和math评估集有什么区别?
7. mbpp和hella swag评估集有什么区别?
8. 阿尔法狗强化学习策略是什么?
9. 提升推理能力和指令跟随能力哪个更难，为什么，提升指令跟随能力的优化方式和其他的比如推理有什么不一样的地方
10. dpo训完了一般输出长度会变化吗?如何解决这个问题
11. 注意力机制为什么除以根号dk，为什么不是dk

    选择除以$ \sqrt{d_k}$ 而不是 $d_k$，是为了在点积注意力中维持数值的稳定性。通过这样的缩放，softmax 函数的输出能够保持适当的范围，防止过大或过小的梯度，进而保证模型的有效训练。
12. transformer里边norm的位置在哪里，norm如何计算的

13.大模型训练过程学习率一般如何变化的，退火阶段学习率如何变化的

代码

1. 写了个注意力层

```python
import torch
import torch.nn as nn
import torch.nn.functional as F

class MultiHeadSelfAttention(nn.Module):
    def __init__(self, embed_size, heads):
        super(MultiHeadSelfAttention, self).__init__()
        self.embed_size = embed_size
        self.heads = heads
        self.head_dim = embed_size // heads

        assert (
            self.head_dim * heads == embed_size
        ), "Embedding size needs to be divisible by heads"

        # 定义 Q, K, V 的线性变换
        self.values = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.keys = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.queries = nn.Linear(self.head_dim, self.head_dim, bias=False)
        self.fc_out = nn.Linear(heads * self.head_dim, embed_size)

    def forward(self, values, keys, query, mask):
        N = query.shape[0]  # 批次大小
        value_len, key_len, query_len = values.shape[1], keys.shape[1], query.shape[1]

        # 将输入分成多个头
        values = values.reshape(N, value_len, self.heads, self.head_dim)
        keys = keys.reshape(N, key_len, self.heads, self.head_dim)
        queries = query.reshape(N, query_len, self.heads, self.head_dim)

        # 计算 QK^T / sqrt(d_k)
        energy = torch.einsum("nqhd,nkhd->nhqk", [queries, keys])
        if mask is not None:
            energy = energy.masked_fill(mask == 0, float("-1e20"))

        # 计算注意力分数
        attention = torch.softmax(energy / (self.head_dim ** (1 / 2)), dim=3)

        # 计算最终的 V 值
        out = torch.einsum("nhql,nlhd->nqhd", [attention, values]).reshape(
            N, query_len, self.heads * self.head_dim
        )

        # 通过最后的全连接层
        out = self.fc_out(out)
        return out
```

2. 手撕，一个数组，输出这个数组每个位置之外的其他元素的乘机，不能用除法，要求尽量减少时间复杂度，然后要求仅用一个数组存储

## transformer系列

1.注意力机制和自注意力机制有什么不同

注意力机制是一个帮助算法辨别信息重要性的计算流程，它通过计算样本与样本之间相关性来判断**每个样本之于一个序列的重要程度**，并**给这些样本赋予能代表其重要性的权重**。很显然，注意力机制能够为样本赋予权重的属性与序列模型研究领域的追求完美匹配，Transformer正是利用了注意力机制的这一特点，从而想到利用注意力机制来进行权重的计算。

> **面试考点**`<br><br>`

作为一种权重计算机制、注意力机制有多种实现形式。经典的注意力机制（Attention）进行的是跨序列的样本相关性计算，这是说，经典注意力机制考虑的是序列A的样本之于序列B的重要程度。这种形式常常用于经典的序列到序列的任务（Seq2Seq），比如机器翻译；在机器翻译场景中，我们会考虑原始语言系列中的样本对于新生成的序列有多大的影响，因此计算的是原始序列的样本之于新序列的重要程度。

`<br><br>`不过在Transformer当中我们使用的是“自注意力机制”（Self-Attention），这是在一个序列内部对样本进行相关性计算的方式，核心考虑的是序列A的样本之于序列A本身的重要程度。

当我们使用注意力机制来分析文本时，自注意力机制可能会为“开心”和“兴奋”这样的词分配更高的权重，因为这些词直接关联到句子的情感倾向。在很长一段时间内、长序列的理解都是深度学习世界的业界难题，在众多研究当中研究者们尝试着从记忆、效率、信息筛选等等方面来寻找出路，而注意力机制所走的就是一条“提效”的道路。**如果我们能够判断出一个序列中哪些样本是重要的、哪些是无关紧要的，就可以引导算法去重点学习更重要的样本，从而可能提升模型的效率和理解能力**。

2.
