{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a043d721-6e5d-48d1-92be-277527a844f9",
   "metadata": {},
   "source": [
    "# Transformer公开课（2）\n",
    "# Transformer架构的PyTorch实现\n",
    "#### @菜菜TsaiTsai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "321715c7-cc1b-41f6-a089-50ceecb7c581",
   "metadata": {},
   "source": [
    "## 课程目录\n",
    "\n",
    "<font color=\"red\">**红色为本节公开课内容，其他为付费正课《深度学习实战》课程内容。加vx号littlecat_1201回复“优惠”，即可了解正课哦！**</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "631d838b-7b83-48e9-ba17-a22987816278",
   "metadata": {},
   "source": [
    "**0 前言**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;0.1 Transformer模型的地位与发展历程<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;0.2 序列模型的基本思路与根本诉求<br>\n",
    "\n",
    "**1 注意力机制**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.1 注意力机制的本质<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.2 Transformer中的自注意力机制运算流程<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.3 Multi-Head Attention 多头注意力机制\n",
    "<br>\n",
    "\n",
    "**2 Transformer的基本结构**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.1 Embedding层与位置编码技术<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.2 Encoder结构解析<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.1 残差连接<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.2 Layer Normalization层归一化<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.3 Feed-Forward Networks前馈网络<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.3 Decoder结构解析<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.1 完整Transformer与Decoder-Only结构的数据流<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2 Encoder-Decoder结构中的Decoder<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.1 输入与teacher forcing<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.2 掩码注意力机制<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.3 普通掩码与前馈掩码<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.4 编码器-解码器注意力层<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.3 Decoder-Only结构中的Decoder<br>\n",
    "\n",
    "**3 Transformer的PyTorch实战**<br>\n",
    "<font color=\"red\">&nbsp;&nbsp;&nbsp;&nbsp;3.1 PyTorch中的Transformer层<br></font>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.2 Encoder-Only任务下的Trnasformer实战<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1 Encoder-Only任务下的Transformer架构<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1.1 Embedding层与Encoder数据输入<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1.2 位置编码的实现与技巧<br>\n",
    "<font color=\"red\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1.3 从0实现编码器Only架构</font><br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.2 【实战】Transformer的情感分类案例<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.3【实战】Decoder-Only架构下的文字生成案例<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.1 数据导入与数据预处理<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.2 Decoder-Only Transformer的架构<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.3 生成式算法的预测与训练<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.4 生成式模型的改进<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.4 Huggingface入门与调用<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.1 Huggingface入门与官网使用指南<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.2 加载并使用预训练模型<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.3 词嵌入工具与词嵌入模型<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.4 全流程自动化的Pipelines工具<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.5 Huggingface中的模型微调<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.4 【实战】Transformer的机器翻译案例<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.5 【实战】Transformer的时间序列案例<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f2412c1-dadd-4a7c-8865-11fd7204786a",
   "metadata": {},
   "source": [
    "## 3.1 PyTorch中的Transformer层"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40055a14-a740-407e-af62-14a6e3aeb121",
   "metadata": {},
   "source": [
    "在之前的课程当中，我们已经认识了PyTorch框架的基本结构，整个PyTorch框架可以大致被分Torch和成熟AI领域两大板块，其中Torch包含各类神经网络组成元素、用于构建各类神经网络，各类AI领域中则包括Torchvision、Torchtext、Torchaudio等辅助完成图像、文字、语音方面各类任务的领域模块。\n",
    "\n",
    "在PyTorch中，Transformer算法是属于“构建循环神经网络的元素”，而非“成熟神经网络”，因此Transformer是位于PyTorch.nn这个基本模块下。为什么PyTorch中的Transformer结构是位于nn，而不是属于成熟神经网络呢？**事实上，在PyTorch中并没有完整的Transformer架构，只有用于构建Transformer的各个层**。我们一起来看一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b71461a-5125-41ee-83a7-cb9fe167429e",
   "metadata": {},
   "source": [
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-1.png\" alt=\"描述文字\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb092dc-194b-410f-803f-20c44efd910f",
   "metadata": {},
   "source": [
    "在torch.nn模块下，存在**服务于Transformer架构的各类神经网络层和模型**，我们来看一下——\n",
    "\n",
    "| 类名称                     | 作用                                          |\n",
    "|--------------------------|---------------------------------------------|\n",
    "| `nn.Transformer`          | 不带输入与输出层的 Transformer 模型，同时具备编码器和解码器                       |\n",
    "| `nn.TransformerEncoder`   | Transformer 编码器的堆叠层，可以控制Nx的N的具体数字                    |\n",
    "| `nn.TransformerDecoder`   | Transformer 解码器的堆叠层，可以控制Nx的N的具体数字                    |\n",
    "| `nn.TransformerEncoderLayer` | Transformer 编码器层，由自注意力和前馈网络组成   |\n",
    "| `nn.TransformerDecoderLayer` | Transformer 解码器层，由自注意力、编码器-解码器注意力和前馈网络组成 |\n",
    "| `nn.MultiheadAttention`   | 多头注意力机制                               |\n",
    "| `nn.LayerNorm`            | 层归一化层                                   |\n",
    "| `nn.Embedding`            | 嵌入层，用于将输入序列转换为嵌入表示          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1488172b-e46f-4bc0-8fa0-c5a2b6ac0606",
   "metadata": {},
   "source": [
    "- **nn.Transformer**\n",
    "\n",
    "`nn.Transformer`封装了完整的Transformer结构。如下图所示，它对Encoder和Decoder两部分的包装，它并没有实现输入中的Embedding和Positional Encoding和最后输出的Linear+softmax部分。\n",
    "\n",
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-37.png\" alt=\"描述文字\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61595ee2-376c-4ac6-9a28-a6d509b79ef1",
   "metadata": {},
   "source": [
    "- **分割的编码器与解码器**\n",
    "\n",
    "`nn.TransformerEncoderLayer`与`nn.TransformerDecoderLayer`: 这两个类表示Transformer单一的编码器和单一的解码器（他们代表了架构图中展示的结构，而不包括Nx的部分）。他们都包含了自注意力机制（self-attention）、多头注意力机制（Multi-head Attention）和前馈网络（feedforward network），以及必要的归一化和残差连接。这两个层的区别在于：\n",
    "> - DecoderLayer默认带有teacher forcing机制，而Encoder layer则没有这个机制。<br><br>\n",
    "> - DecoderLayer带有的Multi-head Attention层可以用来处理编码器-解码器注意力层中的运算，但是EncoderLayer中带有的多头注意力层却没有这个机制。\n",
    "\n",
    "`nn.TransformerEncoder`与`nn.TransformerDecoder`: 这两个类是将单一解码器和单一编码器堆叠后构成的解码器、编码器串，其中`nn.TransformerEncoder`包含了多个nn.TransformerEncoderLayer层的堆叠，`nn.TransformerDecoder`包含了多个nn.TransformerDecoderLayer层的堆叠。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee63c454-ae53-4173-800f-7e0998d447f2",
   "metadata": {},
   "source": [
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-38.png\" alt=\"描述文字\" width=\"800\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e77f7480-a80e-4577-8915-dc4d2be3eb38",
   "metadata": {},
   "source": [
    "除此之外，我们还有：\n",
    "\n",
    "`nn.MultiheadAttention`: 这个模块实现了多头注意力机制，这是Transformer模型的核心组件之一。多头注意力允许模型在不同的位置同时处理来自序列不同部分的信息，这有助于捕捉序列内的复杂依赖关系。\n",
    "\n",
    "`nn.LayerNorm`: 层归一化（Layer Normalization）通常用在Transformer的各个子层的输出上，有助于稳定训练过程，并且提高了训练的速度和效果。\n",
    "\n",
    "`nn.Embedding`：一个预训练好的语义空间，它将每个标记（如单词、字符等）映射到一个高维空间的向量。这使得模型能够处理文本数据，并为每个唯一的标记捕获丰富的语义属性。嵌入层通常是自然语言处理模型的第一层，用于将离散的文本数据转化为连续的向量表示。其输入是索引列表，输出是对应的嵌入向量。\n",
    "\n",
    "`nn.Transformer.generate_square_subsequent_mask`：掩码函数。用于生成一个方形矩阵，用作Transformer模型中自注意力机制的上三角遮罩。这个遮罩确保在序列生成任务中，例如语言模型中，任何给定的元素只会考虑到序列中先于它的元素（即它只能看到过去的信息，不能看到未来的信息）。这种掩码通常在解码器部分使用，防止在预测下一个输出时“作弊”。具体来说，该函数创建了一个方阵，其中对角线及其以下的元素为0（表示可以“看到”这些位置的元素），其余元素为负无穷大（在softmax之前应用，表示位置被屏蔽，不应该有注意力权重）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7f96df92-60dd-4258-b1b0-f5c9c3af1a86",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f7d99abd-ec90-4c49-bf8a-ebeb5382f283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., -inf, -inf, -inf, -inf],\n",
       "        [0., 0., -inf, -inf, -inf],\n",
       "        [0., 0., 0., -inf, -inf],\n",
       "        [0., 0., 0., 0., -inf],\n",
       "        [0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.Transformer.generate_square_subsequent_mask(5) # 5指的是target的维度"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2bb476-402b-4755-a82a-83f9ca8db8b5",
   "metadata": {},
   "source": [
    "| 类名称                     | 作用                                          |\n",
    "|--------------------------|---------------------------------------------|\n",
    "| `nn.Transformer`          | 不带输入与输出层的 Transformer 模型，同时具备编码器和解码器                       |\n",
    "| `nn.TransformerEncoder`   | Transformer 编码器的堆叠层，可以控制Nx的N的具体数字                    |\n",
    "| `nn.TransformerDecoder`   | Transformer 解码器的堆叠层，可以控制Nx的N的具体数字                    |\n",
    "| `nn.TransformerEncoderLayer` | Transformer 编码器层，由自注意力和前馈网络组成   |\n",
    "| `nn.TransformerDecoderLayer` | Transformer 解码器层，由自注意力、编码器-解码器注意力和前馈网络组成 |\n",
    "| `nn.MultiheadAttention`   | 多头注意力机制                               |\n",
    "| `nn.LayerNorm`            | 层归一化层                                   |\n",
    "| `nn.Embedding`            | 嵌入层，用于将输入序列转换为嵌入表示          |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e1f4a3f-7992-4944-a74e-890ef6c64fa7",
   "metadata": {},
   "source": [
    "**在这些所有类中，我们最应该关注的是nn.TransformerEncoderLayer与nn.TransformerDecoderLayer**。这两个层赋予Transformer架构极高的灵活性，大部分时候我们也是会通过这两个层来自定义各种各样丰富的Transformer结构。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c487e9db-52dc-4654-b91d-8edaca5c2a29",
   "metadata": {},
   "source": [
    "- <font color=\"red\">**CLASS`torch.nn.TransformerEncoderLayer`(d_model, nhead, dim_feedforward=2048, dropout=0.1, activation=\\<function relu\\><function relu>, layer_norm_eps=1e-05, batch_first=False, norm_first=False, bias=True, device=None, dtype=None)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791f2c7c-73e8-4a53-a47d-bbfb30d75f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "#(batch_size, seq_len, input_dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dc91784-445c-4f4e-a3f2-ac6d3ac07a6c",
   "metadata": {},
   "source": [
    "| 实例化前-参数名称           | 说明                                                                                     |\n",
    "|--------------------|------------------------------------------------------------------------------------------|\n",
    "| `d_model`          | 输入的嵌入维度（Embedding过程中规定的特征维度），数学公式中的$d_k$                          |\n",
    "| `nhead`            | 多头注意力机制中的头数，在代码中通常表示为num_heads                                           |\n",
    "| `dim_feedforward`  | 前馈网络的隐藏层维度，默认值为 2048。                                                    |\n",
    "| `dropout`          | Dropout 概率，默认值为 0.1。在Transformer架构图中虽然没有展现dropout层，但现在业内习惯于将Dropout层放置在每一个复杂结构之后，在Encoder中，Dropout出现在自注意力层后、残差链接之前，也出现在前馈神经网络后、残差链接之前|\n",
    "| `activation`       | 激活函数，默认值为 `relu`。                                                              |\n",
    "| `layer_norm_eps`   | 层归一化的 epsilon 值，默认值为 1e-05。                                                  |\n",
    "| `batch_first`      | 如果为 `True`，则输入和输出张量的形状为 `(batch_size, seq_len, feature)`，否则为 `(seq_len, batch_size, feature)`。默认值为 `False`。 |\n",
    "| `norm_first`       | 如果为 `True`，则执行前馈网络之前进行层归一化。默认值为 `False`。                        |\n",
    "| `bias`             | 如果为 `True`，则在线性层中使用偏置。默认值为 `True`。                                  |\n",
    "| `device`           | 指定层的设备，默认值为 `None`。                                                         |\n",
    "| `dtype`            | 指定层的数据类型，默认值为 `None`。                                                     |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6a2ef17b-4947-43e7-93c1-d93f99bbc642",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_layer = nn.TransformerEncoderLayer(d_model=512, nhead=8, batch_first=True)\n",
    "src = torch.rand(32, 10, 512)\n",
    "out = encoder_layer(src)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9ed097ef-dc56-4c0d-8a76-3d2c905784c3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape #经过多头注意力机制、残差链接、前馈网络、层归一化，但完全不改变数据结构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9287e91e-4c3d-4a7d-adcb-1b639a841368",
   "metadata": {},
   "outputs": [],
   "source": [
    "前瞻 - 上三角(seq_len, seq_len)\n",
    "填充 - 每张表会有不同的掩码 (batch_size, seq_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42919f36-d4c0-4e56-acf6-6f945fa0b56a",
   "metadata": {},
   "source": [
    "`torch.nn.TransformerEncoderLayer`实例化后可以输入的内容有：\n",
    "\n",
    "| 实例化后-参数名称              | 说明                                                                                                                         |\n",
    "|------------------------------|------------------------------------------------------------------------------------------------------------------------------|\n",
    "| `src`                        | 输入到编码器层的序列（必填）。                                                                                                 |\n",
    "| `src_mask`                   | 输入序列的掩码矩阵（可选），默认接收形状为(seq_len, seq_len)的二维矩阵，通常该参数默认是执行前瞻掩码，在encoder中很少使用。                                                                                                       |\n",
    "| `src_key_padding_mask`       | 输入序列的填充掩码矩阵（可选），默认接收形状为(batch_size, seq_len)的二维矩阵，这个参数只提供给填充掩码使用。                                                                                             |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c780b862-40a0-47af-b385-e9c73420ba46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq, pad_token=0):\n",
    "    # seq: (batch_size, seq_len, embedding_dim)\n",
    "    # 检查填充值位置\n",
    "    padding_mask = (seq == pad_token).all(dim=-1)  # (batch_size, seq_len)\n",
    "    padding_mask = padding_mask.float() * -1e9\n",
    "    \n",
    "    return padding_mask\n",
    "\n",
    "def create_look_ahead_mask(seq_len, start_seq=1):\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len)), diagonal=start_seq)  # 上三角矩阵\n",
    "    mask = mask.float() * -1e9  # 将未来的位置设置为负无穷大\n",
    "    return mask  # (seq_len, seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92fc591d-2108-440e-b989-939eaead1947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "12bb6bb6-f948-4238-a7c7-1625e9c53214",
   "metadata": {},
   "outputs": [],
   "source": [
    "src_key_padding_mask = create_padding_mask(src,pad_token=0)\n",
    "src_mask = create_look_ahead_mask(10,start_seq=1) #其实一般对于encoder中的数据并不会去使用"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "630291c6-8466-4b62-8bf2-4a33d9fd5cdd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_key_padding_mask.shape #batch_size, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "19e2baa4-bcf5-41d9-8bdf-4ed70b98c9bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([10, 10])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "src_mask.shape #seq_len, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5d4d626f-93a7-4e12-bb2b-53d749be4210",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 10, 512])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer(src\n",
    "              ,src_mask = src_mask\n",
    "              , src_key_padding_mask = src_key_padding_mask).shape #结构不变，但数值是增加了掩码的数值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ffe9f41d-a69c-407d-9155-178089052c8d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.6185,  0.9043,  0.2690,  ..., -0.7528, -2.6922, -0.0568],\n",
       "        [ 0.2093, -0.3952, -0.3863,  ..., -1.9490, -1.0719, -0.1734],\n",
       "        [-0.8843, -0.0294, -1.2462,  ..., -1.0629, -0.4550, -0.0064],\n",
       "        ...,\n",
       "        [-0.7402, -1.7728,  0.7999,  ..., -0.9688, -0.4042, -0.5975],\n",
       "        [ 0.0606,  0.5342,  0.0650,  ...,  0.2278, -0.4304,  1.4630],\n",
       "        [-1.1481,  0.0796, -1.1551,  ..., -1.1815, -1.2460, -0.9036]],\n",
       "       grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_layer(src,src_mask = src_mask, src_key_padding_mask = src_key_padding_mask)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27031406-788e-4bd7-898d-4e038061f17d",
   "metadata": {},
   "source": [
    "#### 3.2.1.3 从0实现编码器Only架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "49b7e519-63b1-4f9d-85f5-d1e1e84acec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TransformerEncoderModel(nn.Module):\n",
    "    def __init__(self, input_dim, d_model, nhead, num_encoder_layers, dim_feedforward, dropout=0.1):\n",
    "        \"\"\"\n",
    "        初始化 Transformer 编码器模型\n",
    "        \n",
    "        参数:\n",
    "        input_dim (int): 输入的词汇表大小。\n",
    "        d_model (int): 嵌入向量的维度。\n",
    "        nhead (int): 多头注意力机制中的头数。\n",
    "        num_encoder_layers (int): 编码器层的数量。\n",
    "        dim_feedforward (int): 前馈网络的隐藏层维度。\n",
    "        dropout (float): Dropout 概率。\n",
    "        \"\"\"\n",
    "        super(TransformerEncoderModel, self).__init__()\n",
    "        \n",
    "        # 嵌入层，将输入的词汇索引转换为嵌入向量\n",
    "        self.embedding = nn.Embedding(input_dim, d_model)\n",
    "        \n",
    "        # 位置编码层，添加位置信息以保留序列顺序\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout)\n",
    "        \n",
    "        # 定义单个 Transformer 编码器层\n",
    "        encoder_layers = nn.TransformerEncoderLayer(d_model, nhead, \n",
    "                                                    dim_feedforward, dropout,\n",
    "                                                    batch_first=True)\n",
    "        \n",
    "        # 堆叠多个 Transformer 编码器层\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layers\n",
    "                                                         , num_layers=num_encoder_layers)\n",
    "        \n",
    "        # 保存 d_model 维度，可能会用于后续计算\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # 输出层，将 Transformer 编码器的输出转换为目标任务的输出\n",
    "        # 用于回归任务\n",
    "        self.fc_out = nn.Linear(d_model, 1) \n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # 用于二分类任务\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(d_model, 1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "        # 用于多分类任务\n",
    "        self.fc_out = nn.Sequential(\n",
    "            nn.Linear(d_model, num_classes),\n",
    "            nn.Softmax(dim=1)\n",
    "        )\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "    def forward(self, src, src_mask=None, src_key_padding_mask=None):\n",
    "        \"\"\"\n",
    "        前向传播函数\n",
    "        \n",
    "        参数:\n",
    "        src (Tensor): 进行embedding之前的张量，形状为 (batch_size, seq_len)\n",
    "        src_mask: 前瞻掩码，输入结构为(seq_len, seq_len)\n",
    "        src_key_padding_mask: 填充掩码，输入结构为(batch_size,seq_len)。\n",
    "        \n",
    "        返回值:\n",
    "        Tensor: 模型的输出，形状为 (batch_size, 1)\n",
    "        \"\"\"\n",
    "        # 将输入词汇索引转换为嵌入向量，并进行缩放\n",
    "        # Scaled Embedding = Embedding × sqrt(d_model)\n",
    "        # 这里为什么要进行缩放？\n",
    "        src = self.embedding(src) * torch.sqrt(torch.tensor(self.d_model, dtype=torch.float32))\n",
    "        \n",
    "        # 添加位置编码\n",
    "        src = self.pos_encoder(src)\n",
    "        \n",
    "        # 通过 Transformer 编码器层进行编码\n",
    "        output = self.transformer_encoder(src, mask=src_mask, src_key_padding_mask=src_key_padding_mask)\n",
    "        \n",
    "        # 对编码器的输出进行平均池化，获得序列的固定长度表示\n",
    "        # 这一步同样也是对Transformer输出数据结构的整合\n",
    "        # 如果Encoder的结果是直接输出给Decoder使用，很可能不需要这一步骤\n",
    "        output = output.mean(dim=1)\n",
    "        \n",
    "        # 通过全连接层将固定长度表示转换为目标任务的输出\n",
    "        output = self.fc_out(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc5fee37-9942-467b-822d-6487e3ac0813",
   "metadata": {},
   "source": [
    "在Transformer 模型中，对输入的嵌入向量进行缩放是一个常见的技巧，最早是在原始的 Transformer 论文《Attention Is All You Need》中提出的。我们来详细解释一下这种缩放的意义以及为什么要乘以 `torch.sqrt(torch.tensor(d_model))`。\n",
    "\n",
    "> 缩放的意义\n",
    "> 1. **防止数值过小**：在进行多头注意力机制时，输入的嵌入向量会与注意力权重矩阵相乘，如果嵌入向量的值过小，可能会导致梯度消失问题。通过缩放嵌入向量的值，可以减缓这种问题。<br><br>\n",
    "> 2. **稳定训练过程**：缩放嵌入向量可以使注意力机制的计算更加稳定，从而有助于模型的训练。在论文中提到，通过缩放嵌入向量，可以使模型在训练初期更快地收敛。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "d8960a51-d042-4a0a-bff7-cb44eea716d8",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(22.6274)"
      ]
     },
     "execution_count": 153,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "d_model=512\n",
    "torch.sqrt(torch.tensor(d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "177066bc-cefe-4f63-b28d-7e0bfe3dcb71",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 示例：为Encoder-Only结构输入数据、跑通架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "80dd10fd-c50e-4b99-9f8b-f4caf9a9e4a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 219])"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences.shape #之前在embedding的例子中使用的序列"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "928fa80a-db49-4790-8218-67f10347efd3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[  1,   2,   3,   4,   1,   5,   6,   7,   2,   3,   4,   6,   8,   8,\n",
       "           9,   4,  10,  11,  12,  13,  14,  15,  16,   5,  17,  18,  19,  20,\n",
       "          21,   5,  22,   5,  23,   5,  24,  25,  26,  27,  28,  29,   4,  30,\n",
       "           5,  31,  32,  33,  34,   5,  17,  18,  35,  36,  37,   5,  38,   5,\n",
       "          39,  40,  41,  42,  43,   4,  44,   5,  35,  45,  46,  47,  48,  49,\n",
       "          50,  30,  51,  52,  53,   5,  17,  18,  54,  55,   5,  40,  56,   4,\n",
       "          57,   5,  58,  59,  60,   7,  61,   5,  59,  62,  63,  64,   2,  65,\n",
       "          66,   5,  17,  18,  67,  68,  69,  70,   5,  71,   2,  72,  73,   5,\n",
       "          74,   4,  75,  76,  77,   5,  45,  78,  79,  80,  59,  81,  82,   3,\n",
       "          83,   5,  17,  18,  84,   9,  85,  86,  87,  88,  89,   5,  90,  91,\n",
       "          92,   5,  93,  94,   5,  93,  95,   5,  93,  96,  97,   5,  17,  18,\n",
       "          59,  98,  81,  99, 100,  85, 101,   7, 102,   5, 103,  88, 104, 105,\n",
       "           4, 106, 107,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,\n",
       "           0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
       "        [109,  80, 110, 111, 112, 113, 114,   5, 115, 116, 117, 118, 107, 119,\n",
       "         120,  59, 121, 122, 111,   5, 110, 123,   2, 124,   4,   5,  17,  18,\n",
       "          59, 125,   4,   2, 126, 127,   4, 106,   5, 111, 128, 114, 124, 129,\n",
       "           5,  59,  48, 127, 130, 131, 106, 114,   5,  17,  18, 132, 133, 134,\n",
       "           4, 109, 135, 136, 109, 107, 119, 137,  39, 138, 139, 140,   5, 141,\n",
       "         142,   5, 143, 144,   5,  17,  18, 145,   5, 146, 147,   5, 148, 149,\n",
       "         150, 151, 152,   5,   2, 153, 154, 155,   5, 156,  59,   4, 157,  80,\n",
       "          17,  18,  48, 158, 159, 160, 119, 161, 162, 163,  39,  86, 164,   4,\n",
       "           5,  59, 165,   4, 166, 124,   4, 106,   5, 111, 150, 167, 168, 110,\n",
       "         169,  17,  18, 170,   4, 171, 107, 172, 173, 174, 175,   5, 176, 177,\n",
       "           5,  35, 178, 114, 126,   4, 169, 179,   5, 180,  88, 181, 182,  17,\n",
       "          18, 183, 184, 185, 186,   5, 187, 188,   5, 119, 189, 190,   5, 191,\n",
       "         130, 192,   5, 130, 193,   5, 130, 194,   5, 195,   5,  17,  18, 196,\n",
       "         197,   6,   4,  86, 198, 199,   5, 111,  84, 111, 126,   5, 200, 201,\n",
       "         202, 119, 203,   4,   2,   5, 204,  59, 130,   4,   5,  17,  18, 205,\n",
       "         206, 130,   5, 111, 207, 130, 111, 126, 107]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "c4c214b8-6e29-4b17-aec0-e8ded6b90843",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = 2000  # 词汇表大小\n",
    "batch_size = 1\n",
    "seq_len = 219\n",
    "d_model = 512\n",
    "nhead = 8\n",
    "num_encoder_layers = 6\n",
    "dim_feedforward = 2048\n",
    "dropout = 0.1\n",
    "\n",
    "model = TransformerEncoderModel(input_dim, d_model, nhead\n",
    "                                , num_encoder_layers\n",
    "                                , dim_feedforward, dropout)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "6402b167-4c9c-4af8-ae8e-c74b0b91d49c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模拟掩码 - Encoder-Only结构，无需前瞻掩码，只需填充掩码\n",
    "# 当你的模型输入是(batch_size, seq_len, embedding_dim)时，你的填充掩码函数：\n",
    "def create_padding_mask_1(seq, pad_token=0):\n",
    "    # seq: (batch_size, seq_len, embedding_dim)\n",
    "    # 检查填充值位置\n",
    "    padding_mask = (seq == pad_token).all(dim=-1)  # (batch_size, seq_len)\n",
    "    padding_mask = padding_mask.float() * -1e9\n",
    "    return padding_mask\n",
    "\n",
    "#当你的模型输入是(batch_size, seq_len)时，你的填充掩码函数：\n",
    "def create_padding_mask_2(seq, pad_token=0):\n",
    "    # seq: (batch_size, seq_len)\n",
    "    # 创建一个与输入序列形状相同的掩码\n",
    "    padding_mask = (seq == pad_token).float() * -1e9  # (batch_size, seq_len)\n",
    "    return padding_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "ee1632f1-2963-4ab0-8647-6a484ddf3516",
   "metadata": {},
   "outputs": [],
   "source": [
    "padding_mask = create_padding_mask_2(sequences,pad_token=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "4b637787-c26e-49c5-b903-21d92bb81b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 219])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "padding_mask.shape #batch_size, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "ae4b5a56-1079-45b0-be25-88ffc549368d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "# 前向传播\n",
    "output = model(sequences,src_key_padding_mask = padding_mask)\n",
    "print(output.shape)  # 输出的形状应为 (batch_size, 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
