{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6a7e7100-b21f-470d-ba3e-35c5c29233ed",
   "metadata": {},
   "source": [
    "# Transformer公开课（4）\n",
    "# Decoder-Only架构与文字生成案例数据处理\n",
    "#### @菜菜TsaiTsai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd6b90b5-a0e3-464f-96ef-aa9d729030c3",
   "metadata": {},
   "source": [
    "## 课程目录\n",
    "\n",
    "<font color=\"red\">**红色为本节公开课内容，其他为付费正课《深度学习实战》课程内容。加vx号littlecat_1201回复“优惠”，即可了解正课哦！**</font><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e566827-eb75-4eab-980f-1122de78c162",
   "metadata": {},
   "source": [
    "**0 前言**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;0.1 Transformer模型的地位与发展历程<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;0.2 序列模型的基本思路与根本诉求<br>\n",
    "\n",
    "**1 注意力机制**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.1 注意力机制的本质<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.2 Transformer中的自注意力机制运算流程<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;1.3 Multi-Head Attention 多头注意力机制\n",
    "<br>\n",
    "\n",
    "**2 Transformer的基本结构**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.1 Embedding层与位置编码技术<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.2 Encoder结构解析<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.1 残差连接<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.2 Layer Normalization层归一化<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.2.3 Feed-Forward Networks前馈网络<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;2.3 Decoder结构解析<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.1 完整Transformer与Decoder-Only结构的数据流<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2 Encoder-Decoder结构中的Decoder<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.1 输入与teacher forcing<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.2 掩码注意力机制<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.3 普通掩码与前馈掩码<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.2.4 编码器-解码器注意力层<br>\n",
    "<font color=\"red\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;2.3.3 Decoder-Only结构中的Decoder<br></font>\n",
    "\n",
    "**3 Transformer的PyTorch实战**<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.1 PyTorch中的Transformer层<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.2 Encoder-Only任务下的Trnasformer实战<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1 Encoder-Only任务下的Transformer架构<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1.1 Embedding层与Encoder数据输入<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1.2 位置编码的实现与技巧<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.1.3 从0实现编码器Only架构<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.2.2 【实战】Transformer的情感分类案例<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.3【实战】Decoder-Only架构下的文字生成案例<br>\n",
    "<font color=\"red\">&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.1 数据导入与数据预处理<br></font>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.2 Decoder-Only Transformer的架构<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.3 生成式算法的预测与训练<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.3.4 生成式模型的改进<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.4 Huggingface入门与调用<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.1 Huggingface入门与官网使用指南<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.2 加载并使用预训练模型<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.3 词嵌入工具与词嵌入模型<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.4 全流程自动化的Pipelines工具<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;3.4.5 Huggingface中的模型微调<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.4 【实战】Transformer的机器翻译案例<br>\n",
    "&nbsp;&nbsp;&nbsp;&nbsp;3.5 【实战】Transformer的时间序列案例<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d6fa1cf-8ae5-4fb4-bc4b-28f47b62af77",
   "metadata": {},
   "source": [
    "### 2.3.3 Decoder-Only结构中的Decoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf245fc2-2fd0-42b3-a54f-86b8a8caee04",
   "metadata": {},
   "source": [
    "现在，让我们来看看Decoder-only结构下的Decoder。在Decoder-only结构下的Decoder是专用于生成式任务的架构，它从整个Transformer结构中抽离出来、有独特的训练流程与结构。我们先从结构来看——\n",
    "\n",
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/IUCxP.png\" alt=\"描述文字\" width=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e99753-86df-4f6d-bb72-5f3733a6505b",
   "metadata": {},
   "outputs": [],
   "source": [
    "训练——（teacher forcing - 不会累计错误）\n",
    "\n",
    "这是最好的时代 👉 xxx\n",
    "\n",
    "这是最好的时代，这 👉 xxx\n",
    "\n",
    "这是最好的时代，这是 👉 xxx\n",
    "\n",
    "测试——（autoregressive - 累计错误）\n",
    "\n",
    "这是最坏的时代 👉 xxx\n",
    "\n",
    "这是最坏的时代，xxx 👉 xxx\n",
    "\n",
    "这是最坏的时代，xxxxxx 👉 xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7929c5c0-97fd-43f6-8ea4-9aa3f0d6dfba",
   "metadata": {},
   "source": [
    "如图所示，与原本的Decoder结构相比，Decoder-only状态下的Decoder不再存在编码器-解码器注意力层，整个结构会变得更像编码器Encoder，但依然保留着Teacher forcing和掩码机制。由于没有了编码器-解码器注意力层，因此原本依赖于编码器-解码器注意力层完成的整套训练和运算流程也都不再有效了，相对的，在Decoder-only结构中的Decoder大部分时候都采用“自回归”的训练流程——自回归流程在时间序列预测中是一种常用的方法，它逐步生成未来的值，每一步的预测依赖于前一步的实际值或预测值，而Decoder-only状态下的训练、预测流程都是这样的流程。**在自回归场景中，Decoder的任务是——**\n",
    "\n",
    "1. **利用序列的前半段预测序列的后半段**，因此Decoder的输入数据是一段时间序列、一段文字，输出的是对未来时间的预测、对未来文字的填补<br><br>\n",
    "\n",
    "2. **利用teacher forcing机制和自回归机制的本质，在训练和预测流程中使用标签来辅助预测**。具体地来说，在训练流程中，Decoder利用teacher forcing机制、不断将正确的标签作为特征数据使用；在测试流程中，Decoder利用自回归的属性，将前一步的预测值作为特征数据来使用。\n",
    "\n",
    "<font color=\"red\">**在生成式任务中，一般我们不再区分“特征和标签”这两种不同的数据，在大多数生成式任务中，我们有且只有一种数据——就是需要继续生成、继续补充的那段序列**</font>。生成式任务带有一定的“自监督”属性，我们训练用的数据、和要预测的数据都来自于同一段序列，因此标签数据在下一个时间步就会成为我们的特征数据，故而我们也不会特地再去区分特征和标签、而是会区分“输入”与“输出”。不过，从架构图上来看，除了要预测的序列本身之外，我们依然也可以给Decoder输入更多额外的信息（图上的inputs部分）。大部分时候，我们可以使用这条数据流线路向Decoder传递一些相应的“条件”与“背景知识”，可以帮助我们更好地进行信息的生成和填补。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55393420-7804-4150-be8b-a98d5ea181de",
   "metadata": {},
   "source": [
    "具体来看，Decoder-only状态下的<font color=\"red\">**训练流程**</font>如下，假设需要预测的序列为y，编码好的结果为ebd_y，其中我们取ebd_y的前n个字符作为输入，n个字符后的字符作为标签："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d42a656-e246-4f89-949f-213e30f2516a",
   "metadata": {},
   "outputs": [],
   "source": [
    "训练——（teacher forcing - 不会累计错误）\n",
    "\n",
    "这是最好的时代 👉 xxx\n",
    "\n",
    "这是最好的时代，这 👉 xxx\n",
    "\n",
    "这是最好的时代，这是 👉 xxx\n",
    "\n",
    "测试——（autoregressive - 累计错误）\n",
    "\n",
    "这是最坏的时代 👉 xxx\n",
    "\n",
    "这是最坏的时代，xxx 👉 xxx\n",
    "\n",
    "这是最坏的时代，xxxxxx 👉 xxx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3abf58-8888-40b5-af56-ba54716f8d76",
   "metadata": {},
   "source": [
    "- **第1步，输入 ebd_y[0] >> 输出预测标签yhat[0]，对应真实标签y[0]**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>序列的前半段</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>\"sos\"</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>对应</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>真实标签y<br>序列的后半段</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th>\n",
    "        </tr>\n",
    "        <tr style=\"color:blue;\">\n",
    "          <td>0</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>6</td><td>最坏的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>7</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>8</td><td>\"eos\"</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7375341-33f5-421f-a4e3-a2b9d712fec6",
   "metadata": {},
   "source": [
    "……\n",
    "- **第n+1步，输入 ebd_y[:n] >> 输出预测标签yhat[n]，对应真实标签y[n]**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>序列的前半段</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>\"sos\"</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>对应</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "    <p>真实标签y<br>序列的后半段</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr style=\"color:blue;\">\n",
    "          <td>4</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>6</td><td>最坏的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>7</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>8</td><td>\"eos\"</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f8505f-2219-4fd0-af41-575dd88a58ab",
   "metadata": {},
   "source": [
    "- **第n+2步，输入 ebd_y[:n+1] >> 输出预测标签yhat[n+1]，对应真实标签y[n+1]**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>序列的前半段</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>\"sos\"</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>对应</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>真实标签y<br>序列的后半段</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr style=\"color:blue;\">\n",
    "          <td>5</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>6</td><td>最坏的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>7</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>8</td><td>\"eos\"</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91de99e9-414a-4af6-b555-01e0266a94f3",
   "metadata": {},
   "source": [
    "- **第n+3步，输入 ebd_y[:n+2] >> 输出预测标签yhat[n+2]，对应真实标签y[n+2]**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>序列的前半段</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>\"sos\"</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>6</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>6</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>对应</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>真实标签y<br>序列的后半段</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>这</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>是</td>\n",
    "        </tr>\n",
    "        <tr style=\"color:blue;\">\n",
    "          <td>6</td><td>最坏的</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>7</td><td>时代</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>8</td><td>\"eos\"</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e092589b-b970-42ad-9685-97e95dfaa42c",
   "metadata": {},
   "source": [
    "而在<font color=\"red\">**推理流程**</font>中，Decoder中运行的流程如下所示——"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15fbe0a6-d8d3-44fe-984a-209601aed886",
   "metadata": {},
   "source": [
    "- **第一步，输入 ebd_y（全部的数据） >> 输出下一步的预测标签**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>全部的序列</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d3bbf8-ed5a-434c-a6f3-b499bccffdaa",
   "metadata": {},
   "source": [
    "- **第二步，输入 ebd_y（全部的数据）+ 预测的yhat >> 输出下一步的预测标签**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>全部的序列</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a247ca7e-65ca-4538-85fd-ff5f0f93b00d",
   "metadata": {},
   "source": [
    "- **第三步，输入 ebd_y（全部的数据）+ 预测的yhat >> 输出下一步的预测标签**\n",
    "\n",
    "<table>\n",
    "  <tr>\n",
    "    <td>\n",
    "      <p>输入Decoder<br>全部的序列</p>\n",
    "      <table style=\"color:red;\">\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>0</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>1</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>2</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>3</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>4</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>5</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "    <td><p>预测出</p>➡\n",
    "    </td>\n",
    "    <td>\n",
    "      <p>当前时间步的预测标签yhat</p>\n",
    "      <table>\n",
    "        <tr>\n",
    "          <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "        </tr>\n",
    "        <tr>\n",
    "          <td>6</td><td>yyy</td><td>0.5621</td><td>0.8920</td><td>0.7312</td><td>0.2543</td><td>0.1289</td>\n",
    "        </tr>\n",
    "      </table>\n",
    "    </td>\n",
    "  </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06f64532-2669-4a0b-9f0c-778712025616",
   "metadata": {},
   "source": [
    "以此类推，直到预测出“eos”后停止。**与Transformer中的Decoder一致，训练流程是可以并行的，这一点通过带掩码的注意力机制来实现**。而推理流程是必须严格遵守自回归要求的、在下一个时间步预测之前必须将上一个时间步的结果计算出来，因此**推理流程中则需要使用循环的方式**来进行预测。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e0c502-c4ff-458a-bab6-30db4f9c9803",
   "metadata": {},
   "source": [
    "- **不从第一个样本开始训练的流程如何实现？**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08ae610b-2fd3-4438-b053-60f39c22ac0e",
   "metadata": {},
   "source": [
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/IUCxP.png\" alt=\"描述文字\" width=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68ff03e5-aa67-474e-a018-f6d24f5dd11a",
   "metadata": {},
   "source": [
    "从Decoder的掩码注意力层中输出的是经过掩码后、每一行只携带特定时间段信息的结果$C_{decoder}$：\n",
    "\n",
    "$$\n",
    "C_{decoder} = \\begin{bmatrix}\n",
    "a_{11}v_{1} & a_{11}v_{1} & \\ldots & a_{11}v_{1} \\\\\n",
    "a_{21}v_{1} + a_{22}v_{2} & a_{21}v_{1} + a_{22}v_{2} & \\ldots & a_{21}v_{1} + a_{22}v_{2} \\\\\n",
    "a_{31}v_{1} + a_{32}v_{2} + a_{33}v_{3} & a_{31}v_{1} + a_{32}v_{2} + a_{33}v_{3} & \\ldots & a_{31}v_{1} + a_{32}v_{2} + a_{33}v_{3} \\\\\n",
    "a_{41}v_{1} + a_{42}v_{2} + a_{43}v_{3} + a_{44}v_{4} & a_{41}v_{1} + a_{42}v_{2} + a_{43}v_{3} + a_{44}v_{4} & \\ldots & a_{41}v_{1} + a_{42}v_{2} + a_{43}v_{2} + a_{44}v_{4}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5e28764-ccd5-402f-abbc-72ab1f583918",
   "metadata": {},
   "source": [
    "**当我们使用覆盖的时间点来作为脚标**，则有：\n",
    "\n",
    "$$\n",
    "C_{decoder} = \\begin{bmatrix}\n",
    "c_{1} & c_{1} & \\ldots & c_{1} \\\\\n",
    "c_{1 \\to 2} & c_{1 \\to 2} & \\ldots & c_{1 \\to 2} \\\\\n",
    "c_{1 \\to 3} & c_{1 \\to 3} & \\ldots & c_{1 \\to 3} \\\\\n",
    "c_{1 \\to 4} & c_{1 \\to 4} & \\ldots & c_{1 \\to 4}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<font color=\"red\">**同样的，这里出于教学目的，省略了特征维度上的脚标。现在你所看到的脚标只代表时间维度/序列长度的维度。**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b99174-b94c-4ee9-8b5c-9f7c884ab637",
   "metadata": {},
   "source": [
    "此时你会发现，我们必须从单词1开始预测，其流程为：\n",
    "\n",
    "> 单词1 用于预测 单词2<br><br>\n",
    "> 单词1、2 用于预测 单词3<br><br>\n",
    "> 单词1、2、3 用于预测 单词4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc29be0-1f7c-4a98-a576-1dde7699fdc4",
   "metadata": {},
   "source": [
    "但事实上，在生成式的例子中，我们可能会倾向于一开始就给与比较多的信息。我们真正要做的是“利用句子的前半段”去预测“句子的后半段”，大部分时候我们其实很少使用简单的几个单词、或1个单词来进行训练。而是倾向于使用下面的流程——\n",
    "\n",
    "> 单词1:n 用于预测 单词n+1<br><br>\n",
    "> 单词1:n+1 用于预测 单词n+2<br><br>\n",
    "> 单词1:n+2 用于预测 单词n+3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08449ba8-4922-4b29-80c7-a2e6e69db3fa",
   "metadata": {},
   "source": [
    "如果要实现上面的流程，可以怎么做呢？可以通过移动前瞻掩码矩阵的对角线来实现——"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "c6325efc-c17b-472d-9afd-aa7c891649b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "def create_look_ahead_mask(seq_len, start_seq = 1):\n",
    "    mask = torch.triu(torch.ones((seq_len, seq_len)),diagonal=start_seq)  # triu 左下方的三角矩阵，diagonal控制对角线位置\n",
    "    #mask = mask.float() * -1e9  # 将未来的位置设置为负无穷大\n",
    "    return mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "48dd249f-98db-49bb-9ab9-26ce8fe3a414",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_look_ahead_mask(10) #为了教学方便，现在展示的是1和0，实际应该是右上角负无穷，左下角0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "1cfd9e72-c84f-46a1-b120-fdc9e6b5bcc5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]])"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "create_look_ahead_mask(10,start_seq=4) #通过调节对角线，可以让掩码的区域缩小，从而可以允许更多信息的注入"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a516d4-ce4c-4076-9105-0a2185383bdc",
   "metadata": {},
   "source": [
    "当前瞻掩码从第一个时间步开始时，掩码注意力层输出的结果覆盖的时间步为：\n",
    "\n",
    "$$\n",
    "C_{decoder} = \\begin{bmatrix}\n",
    "c_{1} & c_{1} & \\ldots & c_{1} \\\\\n",
    "c_{1 \\to 2} & c_{1 \\to 2} & \\ldots & c_{1 \\to 2} \\\\\n",
    "c_{1 \\to 3} & c_{1 \\to 3} & \\ldots & c_{1 \\to 3} \\\\\n",
    "c_{1 \\to 4} & c_{1 \\to 4} & \\ldots & c_{1 \\to 4}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40c65bd-3883-4cc0-aa83-cbed3128dc79",
   "metadata": {},
   "source": [
    "当前瞻掩码从第4个时间步开始时，掩码注意力层输出的结果覆盖的时间步为：\n",
    "\n",
    "$$\n",
    "C_{decoder} = \\begin{bmatrix}\n",
    "c_{1 \\to 4} & c_{1 \\to 4} & \\ldots & c_{1 \\to 4} \\\\\n",
    "c_{1 \\to 5} & c_{1 \\to 5} & \\ldots & c_{1 \\to 5} \\\\\n",
    "c_{1 \\to 6} & c_{1 \\to 6} & \\ldots & c_{1 \\to 6} \\\\\n",
    "c_{1 \\to 7} & c_{1 \\to 7} & \\ldots & c_{1 \\to 7}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "917ef92b-ac88-4270-87c9-5c15641e0710",
   "metadata": {},
   "source": [
    "这样可以第一次预测过程中所使用的标签为“前n个字”而不是“第一个字”。当然，这已经是属于“自定义掩码”的范围，在实际中并不多见。但通过这种掩码方式，可以要求解码器产出的注意力分数完整接收前几个字之间的相互关系、从而一开始就使用“前半段话”来进行训练。在之后实现Decoder-only预测的过程中，我们将会更详细地讲解这个流程。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13c2455a-c0bd-4d09-ad22-ebccb4516cc3",
   "metadata": {},
   "source": [
    "在探索了Transformer模型的全貌之后，我们可以看到这一架构之所以在自然语言处理和其他序列处理任务中表现卓越，归功于其创新的设计和高效的信息处理能力。从自注意力机制到编码器和解码器的层叠结构，每一部分都精心设计以最大化上下文信息的利用，并提高计算的并行性。Transformer不仅改变了我们处理文本的方式，也为机器学习领域提供了一种强大的工具，用以解决一系列复杂的序列建模问题。\n",
    "\n",
    "自注意力机制使模型能够灵活地捕捉序列内的长距离依赖，而无需依赖于递归网络结构，从而避免了梯度消失和计算效率低下的问题。编码器层通过逐层处理输入数据，有效地提取和聚合信息；而解码器层则利用编码器的输出，结合自回归的方式逐步构建输出序列。通过这种方式，Transformer能够在翻译、文本生成、摘要等任务中生成准确且连贯的文本。\n",
    "\n",
    "此外，编码器-解码器注意力机制是理解输入与输出之间复杂关系的关键，它使得模型能够在生成每个输出时都考虑到与输入序列的具体关联。这种能力使得Transformer不仅适用于传统的NLP任务，还可以扩展到如图像处理和多模态任务中，展示了其极大的灵活性和广泛的适用性。\n",
    "\n",
    "总的来说，Transformer的出现标志着深度学习在处理序列数据方面的一个重大进步。随着研究的深入和技术的发展，我们期待看到更多基于Transformer的创新应用，这将进一步推动人工智能领域的边界向前发展。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3758b27c-8eb7-4508-afab-84e3f5d0f1e7",
   "metadata": {},
   "source": [
    "### 3.3.1 数据导入与数据预处理"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43f55f9-c7b8-44d8-93a7-78e3b7781f49",
   "metadata": {},
   "source": [
    "> 数据导入"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6fe9158c-2cc1-4009-9ec3-b31ff8b1f7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import jieba\n",
    "import os\n",
    "import re\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "698c08a9-4201-4d06-a682-1a39fcb84ecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_file(file_path):\n",
    "    data = []\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        for line in file:\n",
    "            parts = line.strip().split('\\t')\n",
    "            if len(parts) == 2:\n",
    "                label, content = parts\n",
    "                data.append(content) #只要content，不要label\n",
    "    return data\n",
    "\n",
    "import numpy as np\n",
    "class calculate_stats:\n",
    "    def __init__(self,data):\n",
    "        self.total_samples = len(data)\n",
    "        self.len_ = []\n",
    "        for content in data:\n",
    "            self.len_.append(len(content))\n",
    "\n",
    "        self.lower_quartile = np.percentile(self.len_, 25)\n",
    "        self.median = np.median(self.len_)\n",
    "        self.upper_quartile = np.percentile(self.len_, 75)\n",
    "        self.percentile_90 = np.percentile(self.len_, 90)\n",
    "\n",
    "    def stats(self):\n",
    "        # 输出结果\n",
    "        print(f\"总字数: {sum(self.len_)}\")\n",
    "        print(f\"样本数量: {self.total_samples}\")\n",
    "        print(f\"平均每篇文章的字数: {sum(self.len_)/self.total_samples}\")\n",
    "        print(f\"最长句子的字数:{max(self.len_)}\")\n",
    "        print(f\"最短句子的字数:{min(self.len_)}\")\n",
    "        print(f\"句子长度的25%分位数:{self.lower_quartile}\")\n",
    "        print(f\"句子长度的50%分位数:{self.median}\")\n",
    "        print(f\"句子长度的75%分位数:{self.upper_quartile}\")\n",
    "        print(f\"句子长度的90%分位数:{self.percentile_90}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b2317d80-d1c1-4bba-aa20-a9e4d2c74015",
   "metadata": {},
   "outputs": [],
   "source": [
    "PATH = r'DLdata/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "31e67267-1468-40cb-801a-e0c2ff13f2a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = pd.read_csv(os.path.join(PATH,\"cnews.train.txt\")\n",
    "                         , sep=\"\\t\", names = [\"label\",\"content\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ab00c9c0-e94b-4999-a199-3fdb7a7f7780",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1243/2812097312.py:3: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  sampled_df = train_data.groupby('label').apply(lambda x: x.sample(n=2000)).reset_index(drop=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled data saved to DLdata/cnews_train_sampled_2000.txt\n"
     ]
    }
   ],
   "source": [
    "# 随机抽样 - 训练集\n",
    "np.random.seed(1412)\n",
    "sampled_df = train_data.groupby('label').apply(lambda x: x.sample(n=2000)).reset_index(drop=True)\n",
    "\n",
    "# 保存为txt文件\n",
    "output_file_path = os.path.join(PATH,\"cnews_train_sampled_2000.txt\")\n",
    "with open(output_file_path, 'w', encoding='utf-8') as f:\n",
    "    for index, row in sampled_df.iterrows():\n",
    "        f.write(f\"{row['label']}\\t{row['content']}\\n\")\n",
    "\n",
    "print(f\"Sampled data saved to {output_file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64385eca-f358-4cc0-9e52-43bdf8d22d16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 读取数据\n",
    "file_path = r\"DLdata/cnews_train_sampled_2000.txt\"\n",
    "\n",
    "# 读取数据\n",
    "data = read_file(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7b7f9838-43d0-4f77-a38a-7c19d5def9a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 2 samples: ['新浪正在视频直播尼克斯vs魔术 魔兽小斯强强对话新浪体育讯12月31日8:00，新浪体育将为您视频直播魔术主场迎战尼克斯的比赛。摆脱了赛季初的低迷之后，尼克斯打出14胜1负战绩，最近他们在圣诞大战中又战胜了公牛，不过随后一战却再次被热火打败。如今尼克斯两胜公牛，两败于凯尔特人和热火，东部四强中只有魔术还没交手，两队在11月3日曾被安排一战，但是因故未能进行，急欲给自己加盖强队标签的尼克斯会在这场迟来的比赛中全力以赴。而最近4连胜的魔术也想在这场比赛中一试牛刀，连胜凯尔特人马刺的他们，何惧尼克斯？(新体)[视频直播室] [视频直播室-教育网专用] [图文直播室]', '弗老大同意终止合同 高层确认为球队利益让他离去新浪体育讯北京时间12月27日，来自新华网英文版消息，在经历了两周的效力之后，弗朗西斯决定离开北京队，俱乐部高层对此也做了确认。北京队助理教练袁超对新华社说，“弗朗西斯下午来到首钢体育馆，告诉球队，他已经决定离开了。”33岁的弗朗西斯在上一轮对阵江苏队的比赛中，没有能出场，他在中场休息时间无故离开了更衣室。在赛后的新闻发布会上，闵鹿蕾确认了弗朗西斯中场离开的消息，并且说“这是我第一次看到有球员在比赛期间离开的。”闵鹿蕾的一番话，更加加剧了弗朗西斯离开北京队的可能性，而且他在25号缺席了球队的训练，原因是要和家里人度过圣诞节，他在接受采访时候表示：“我没有无故不训练，我给教练打过招呼了，他也答应了。”袁超在接受新华社采访时候，终于说出了今天谈判的进展，“我今天早上和弗朗西斯谈了谈关于他中场离开和圣诞节不训练的事儿，我告诉他，为了球队的利益，我们想让他离开。当时他在会谈中没有给我一个明确的答复。”“但是，当他下午出现在首钢训练馆中的时候，他说他已经准备好要离开了。”袁超说。无论北京俱乐部还是弗朗西斯，都希望双方有个圆满的结局。过去的两周，弗朗西斯一共为北京打了4场比赛，场均3分钟内得到0.5分，0.7个篮板，这和昔日的三次NBA全明星队员相比，确实相差甚远。俱乐部做出这样的决定，或许对双方都有好处。(FRANK)']\n"
     ]
    }
   ],
   "source": [
    "# 查看前2个样本\n",
    "print(f\"First 2 samples: {data[:2]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "977ea874-cfbb-4198-bf67-b900ca5cb847",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "813c75f5-d958-44c0-b9af-2e417516092d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 计算总字符数和样本数\n",
    "cal = calculate_stats(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3d700efe-c8e4-437e-86f0-a6850682420c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总字数: 18272972\n",
      "样本数量: 20000\n",
      "平均每篇文章的字数: 913.6486\n",
      "最长句子的字数:27467\n",
      "最短句子的字数:14\n",
      "句子长度的25%分位数:345.0\n",
      "句子长度的50%分位数:682.0\n",
      "句子长度的75%分位数:1150.25\n",
      "句子长度的90%分位数:1894.0\n"
     ]
    }
   ],
   "source": [
    "cal.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dd64ece-11a6-4c36-ace3-d4b6056d72eb",
   "metadata": {},
   "source": [
    "> 句子分割"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "dee4d3fd-6f09-436e-8fd6-cc52d6797f67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "\n",
    "def sentence_split(text):\n",
    "    sentence_separators = ['。', '！', '？', '；', '…', '：','”',' ',]\n",
    "    sentences = []\n",
    "    start = 0\n",
    "    for i, char in enumerate(text):\n",
    "        if char in sentence_separators:\n",
    "            sentences.append(text[start:i + 1])\n",
    "            start = i + 1\n",
    "    if start < len(text):\n",
    "        sentences.append(text[start:])\n",
    "    return sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "40e6918e-c8d9-4d5d-831a-b0d461946902",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['新浪正在视频直播尼克斯vs魔术 ',\n",
       " '魔兽小斯强强对话新浪体育讯12月31日8:00，新浪体育将为您视频直播魔术主场迎战尼克斯的比赛。',\n",
       " '摆脱了赛季初的低迷之后，尼克斯打出14胜1负战绩，最近他们在圣诞大战中又战胜了公牛，不过随后一战却再次被热火打败。',\n",
       " '如今尼克斯两胜公牛，两败于凯尔特人和热火，东部四强中只有魔术还没交手，两队在11月3日曾被安排一战，但是因故未能进行，急欲给自己加盖强队标签的尼克斯会在这场迟来的比赛中全力以赴。',\n",
       " '而最近4连胜的魔术也想在这场比赛中一试牛刀，连胜凯尔特人马刺的他们，何惧尼克斯？',\n",
       " '(新体)[视频直播室] ',\n",
       " '[视频直播室-教育网专用] ',\n",
       " '[图文直播室]']"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_split(data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dd0e4e7a-8f56-4505-ae97-79fa045dcbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#句子降维\n",
    "sentence_splited = [sentence_split(sentences) for sentences in data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d44654ce-2a40-4f67-b705-a674e8ee6879",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['新浪正在视频直播尼克斯vs魔术 ', '魔兽小斯强强对话新浪体育讯12月31日8:00，新浪体育将为您视频直播魔术主场迎战尼克斯的比赛。', '摆脱了赛季初的低迷之后，尼克斯打出14胜1负战绩，最近他们在圣诞大战中又战胜了公牛，不过随后一战却再次被热火打败。', '如今尼克斯两胜公牛，两败于凯尔特人和热火，东部四强中只有魔术还没交手，两队在11月3日曾被安排一战，但是因故未能进行，急欲给自己加盖强队标签的尼克斯会在这场迟来的比赛中全力以赴。', '而最近4连胜的魔术也想在这场比赛中一试牛刀，连胜凯尔特人马刺的他们，何惧尼克斯？', '(新体)[视频直播室] ', '[视频直播室-教育网专用] ', '[图文直播室]']\n"
     ]
    }
   ],
   "source": [
    "for item in sentence_splited:\n",
    "    print(item)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e5a37517-a5cf-44ef-93ab-f132e82ad8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_list = [item for sublist in sentence_splited for item in sublist]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5338219f-0520-4567-990e-bb2b685c7133",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['新浪正在视频直播尼克斯vs魔术 ',\n",
       " '魔兽小斯强强对话新浪体育讯12月31日8:00，新浪体育将为您视频直播魔术主场迎战尼克斯的比赛。',\n",
       " '摆脱了赛季初的低迷之后，尼克斯打出14胜1负战绩，最近他们在圣诞大战中又战胜了公牛，不过随后一战却再次被热火打败。',\n",
       " '如今尼克斯两胜公牛，两败于凯尔特人和热火，东部四强中只有魔术还没交手，两队在11月3日曾被安排一战，但是因故未能进行，急欲给自己加盖强队标签的尼克斯会在这场迟来的比赛中全力以赴。',\n",
       " '而最近4连胜的魔术也想在这场比赛中一试牛刀，连胜凯尔特人马刺的他们，何惧尼克斯？',\n",
       " '(新体)[视频直播室] ',\n",
       " '[视频直播室-教育网专用] ',\n",
       " '[图文直播室]',\n",
       " '弗老大同意终止合同 ',\n",
       " '高层确认为球队利益让他离去新浪体育讯北京时间12月27日，来自新华网英文版消息，在经历了两周的效力之后，弗朗西斯决定离开北京队，俱乐部高层对此也做了确认。']"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_list[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "986c487f-cc93-4593-b948-32f80f4e9407",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "617810"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_list.__len__()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeff665-2d8e-4bd7-a630-05ef3008f37c",
   "metadata": {},
   "source": [
    "> 句子长度筛选"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b3c65ebb-a987-48b9-9457-f1f85cfd8ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_sentences_by_length(sentences, min_len, max_len):\n",
    "    \"\"\"\n",
    "    筛选出字数超过指定最小长度的所有句子。\n",
    "\n",
    "    参数：\n",
    "    sentences (list of str): 输入的句子列表。\n",
    "    min_length (int): 最小字数长度。\n",
    "\n",
    "    返回：\n",
    "    List[str]: 筛选后的句子列表。\n",
    "    \"\"\"\n",
    "    filtered_sentences = [sentence for sentence in sentences if len(sentence) > min_len and len(sentence) < max_len]\n",
    "    return filtered_sentences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "368ec64f-f753-4d15-9e65-86d321e640c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#筛选掉过短的句子\n",
    "filtered_data = filter_sentences_by_length(merged_list,50,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f2310acb-678b-418e-8ee9-be1187911223",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "110887"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data.__len__()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "475b95e7-ac21-4bd0-9e29-5e4794b76967",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['摆脱了赛季初的低迷之后，尼克斯打出14胜1负战绩，最近他们在圣诞大战中又战胜了公牛，不过随后一战却再次被热火打败。',\n",
       " '如今尼克斯两胜公牛，两败于凯尔特人和热火，东部四强中只有魔术还没交手，两队在11月3日曾被安排一战，但是因故未能进行，急欲给自己加盖强队标签的尼克斯会在这场迟来的比赛中全力以赴。',\n",
       " '高层确认为球队利益让他离去新浪体育讯北京时间12月27日，来自新华网英文版消息，在经历了两周的效力之后，弗朗西斯决定离开北京队，俱乐部高层对此也做了确认。',\n",
       " '在赛后的新闻发布会上，闵鹿蕾确认了弗朗西斯中场离开的消息，并且说“这是我第一次看到有球员在比赛期间离开的。',\n",
       " '闵鹿蕾的一番话，更加加剧了弗朗西斯离开北京队的可能性，而且他在25号缺席了球队的训练，原因是要和家里人度过圣诞节，他在接受采访时候表示：',\n",
       " '袁超在接受新华社采访时候，终于说出了今天谈判的进展，“我今天早上和弗朗西斯谈了谈关于他中场离开和圣诞节不训练的事儿，我告诉他，为了球队的利益，我们想让他离开。',\n",
       " '过去的两周，弗朗西斯一共为北京打了4场比赛，场均3分钟内得到0.5分，0.7个篮板，这和昔日的三次NBA全明星队员相比，确实相差甚远。',\n",
       " '他让整个世界都不同了新浪体育讯北京时间1月16日(美国当地时间1月15日)消息，休斯敦火箭客场挑战亚特兰大老鹰，火箭克服了种种不利因素，最终以112-106战胜了对手。',\n",
       " '和昨天的怒批裁判不同，今天阿帅开起了记者的玩笑，对《休斯敦纪实报》记者费根说，“你跟队报道比赛，你告诉我(赢球的)原因吧。',\n",
       " '对于今天的比赛，阿德尔曼总结道，“我们过去两年，我告诉他们，应该坚持下去，我们有连续赢得比赛的潜力，我们需要像今晚一样终结比赛，今天大家都很努力，阿隆今天打出了一场很好的比赛，手感很好，在昨晚经历了那样一场比赛后，我为他们今晚的表现骄傲。']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_data[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8944323-49ea-473d-b24f-46b2873ee6a7",
   "metadata": {},
   "source": [
    "> 分词、起始符号、终止符号、分chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d8778ac9-7c28-4158-8f1d-e4e5178579fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.674 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "#进行分词，添加起始符号与终止符号\n",
    "data_split = [jieba.lcut(sentence) for sentence in filtered_data]\n",
    "\n",
    "#为现有的句子添加起始符号与终止符号\n",
    "processed_data = []\n",
    "\n",
    "for content in data_split:\n",
    "    content = [\"<sos>\"] + content + [\"<eos>\"]\n",
    "    processed_data.append(content)\n",
    "\n",
    "def merge_and_chunk(data, chunk_size):\n",
    "    \"\"\"\n",
    "    将所有嵌套列表合并为一个长列表，然后按指定大小分块。\n",
    "    \n",
    "    参数：\n",
    "    data (list of list of str): 输入的嵌套字符串列表。\n",
    "    chunk_size (int): 每个块的最大大小。\n",
    "    \n",
    "    返回：\n",
    "    list of list of str: 分块后的字符串列表。\n",
    "    \"\"\"\n",
    "    # 合并所有列表为一个长列表\n",
    "    merged_list = []\n",
    "    for sublist in data:\n",
    "        merged_list.extend(sublist)\n",
    "    \n",
    "    # 通过索引的方式，按指定大小分块\n",
    "    chunks = [merged_list[i:i + chunk_size] for i in range(0, len(merged_list), chunk_size)]\n",
    "    return chunks\n",
    "\n",
    "chunks = merge_and_chunk(processed_data,512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4a49d44b-28aa-408a-86f9-902e1eb7245c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<sos>', '摆脱', '了', '赛季', '初', '的', '低迷', '之后', '，', '尼克斯', '打出', '14', '胜', '1', '负', '战绩', '，', '最近', '他们', '在', '圣诞', '大战', '中', '又', '战胜', '了', '公牛', '，', '不过', '随后', '一战', '却', '再次', '被', '热火', '打败', '。', '<eos>', '<sos>', '如今', '尼克斯', '两胜', '公牛', '，', '两败', '于', '凯尔特人', '和', '热火', '，']\n"
     ]
    }
   ],
   "source": [
    "print(chunks[0][:50])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "796b4145-2839-41b3-b565-e9b072b11d52",
   "metadata": {},
   "source": [
    "> 建词汇表、编码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "44737098-9497-4277-b9b9-4fd5244fcf39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词频统计\n",
    "from collections import Counter\n",
    "\n",
    "flattened_data = [word for sublist in chunks for word in sublist]\n",
    "\n",
    "# 统计词频\n",
    "word_counts = Counter(flattened_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "0d7aea2b-43fb-4197-87d3-fdca5ca3ded0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "，: 386450\n",
      "的: 266212\n",
      "<sos>: 110887\n",
      "<eos>: 110887\n",
      "。: 92382\n",
      "、: 61295\n",
      "在: 57557\n",
      "了: 44477\n",
      "是: 41607\n",
      "和: 33638\n",
      "也: 22747\n",
      "有: 21299\n",
      "》: 19648\n",
      "《: 19477\n",
      "中: 15748\n",
      "都: 15303\n",
      "“: 14289\n",
      "月: 14075\n",
      "): 13909\n",
      "他: 13758\n",
      "将: 13711\n",
      "年: 13515\n",
      "为: 13499\n",
      "我们: 13434\n",
      "(: 13358\n",
      "对: 13252\n",
      "我: 12986\n",
      "一个: 12655\n",
      "就: 12538\n",
      "上: 12490\n"
     ]
    }
   ],
   "source": [
    "# 打印词频最高的前100个词\n",
    "for word, freq in word_counts.most_common(30):\n",
    "    print(f\"{word}: {freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "2509c3b7-799d-476b-a65d-074336d97041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 将高频词保存为列表，将这些词的频率也保存为列表\n",
    "high_freq_word = []\n",
    "high_freq = []\n",
    "for word, freq in word_counts.most_common(100):\n",
    "    high_freq_word.append(word)\n",
    "    high_freq.append(freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a110a375-e604-4b43-9b93-4eb58c940559",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 将所有的词频保存成列表，绘制频率分布图\n",
    "all_freq = list(word_counts.values())\n",
    "all_freq.sort()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# 绘制直方图\n",
    "# 绘制概率密度图\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.kdeplot(all_freq, color='blue', fill=True)\n",
    "plt.title('Log-Transformed Frequency Density of Values')\n",
    "plt.xlabel('Log(Value + 1)')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "2317dee4-af5c-48ec-8e5c-cbfce63c8a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict, Counter\n",
    "\n",
    "class Vocab:\n",
    "    \"\"\"\n",
    "    可以同时接纳Token和text两种类型的数据\n",
    "    对原始文字数据，调用build方法，进行分词、完成预处理、完成词频筛选\n",
    "    对Token数据，使用init中的流程，完成添加未知词、词汇表构建并根据词汇表进行编码\n",
    "    建好词汇表后，再调用单独的方法来进行编码\n",
    "    \"\"\"\n",
    "    def __init__(self, tokens=None):\n",
    "        self.idx_to_token = list()\n",
    "        self.token_to_idx = dict()\n",
    "\n",
    "        if tokens is not None:\n",
    "            if \"<unk>\" not in tokens:\n",
    "                tokens = [\"<unk>\"] + tokens \n",
    "            if \"<sos>\" not in tokens:\n",
    "                tokens = [\"<sos>\"] + tokens\n",
    "            if \"<eos>\" not in tokens:\n",
    "                tokens = [\"<eos>\"] + tokens\n",
    "            for token in tokens:\n",
    "                self.idx_to_token.append(token)\n",
    "                self.token_to_idx[token] = len(self.idx_to_token) - 1\n",
    "            self.unk = self.token_to_idx['<unk>']\n",
    "\n",
    "    @classmethod\n",
    "\n",
    "    def build(cls, text\n",
    "              , min_freq = 1\n",
    "              , stopwords = set([\"的\", \"和\", \"了\", \"在\", \"是\", \"就\", \"不\", \"也\", \"有\", \"但\"])\n",
    "              , preprocessing=False\n",
    "              , reserved_tokens=None):\n",
    "        token_freqs = defaultdict(int)\n",
    "        for tokens in text:\n",
    "            if preprocessing:\n",
    "                #去除标点符号\n",
    "                tokens = [re.sub(r'[^\\w\\s]', '', token) for token in tokens]\n",
    "                #去除停用词\n",
    "                tokens = [token for token in tokens if token and token not in stopwords]\n",
    "            #词频筛选\n",
    "            for token in tokens:\n",
    "                token_freqs[token] += 1\n",
    "        uniq_tokens = [\"<unk>\", \"<sos>\", \"<eos>\"] + (reserved_tokens if reserved_tokens else [])\n",
    "        uniq_tokens += [token for token, freq in token_freqs.items() if freq >= min_freq and token != \"<unk>\"]\n",
    "        return cls(uniq_tokens)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.idx_to_token)\n",
    "\n",
    "    def __getitem__(self, token):\n",
    "        return self.token_to_idx.get(token, self.unk)\n",
    "\n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        return [self[token] for token in tokens]\n",
    "\n",
    "    def convert_ids_to_tokens(self, indices):\n",
    "        return [self.idx_to_token[index] for index in indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7d25a4ad-2282-441b-b573-7b729c59bdfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = Vocab.build(chunks,min_freq=1\n",
    "                   ,preprocessing = False\n",
    "                   )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "dc6391ce-14aa-49e8-af39-da4c3189b8a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "ordinal_token = []\n",
    "for tokens in chunks:\n",
    "    ordinal_token.append(vocab.convert_tokens_to_ids(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d9446e42-52d6-4cf6-8809-08ab15d52367",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "总字数: 4896774\n",
      "样本数量: 9565\n",
      "平均每篇文章的字数: 511.94709879769994\n",
      "最长句子的字数:512\n",
      "最短句子的字数:6\n",
      "句子长度的25%分位数:512.0\n",
      "句子长度的50%分位数:512.0\n",
      "句子长度的75%分位数:512.0\n",
      "句子长度的90%分位数:512.0\n"
     ]
    }
   ],
   "source": [
    "cal = calculate_stats(ordinal_token)\n",
    "cal.stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0183091-3dd6-4308-9bad-86e1fd7e0c69",
   "metadata": {},
   "source": [
    "- **生成式算法/Decoder-Only结构中的数据导入**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3c26508-c65b-49d8-bd0f-1531accc4d74",
   "metadata": {},
   "source": [
    "在数据进入Pytorch之前，我们通常需要将数据进行如下的处理——\n",
    "\n",
    "1. **将数据转变为与PyTorch兼容的结构**，包括但不限于：\n",
    "> - 确保数据集拥有`__len__` 方法（返回数据集大小）和 `__getitem__` 方法（根据索引返回数据）。<br><br>\n",
    "> - 数据应该是 `torch.Tensor` 类型，因为 PyTorch 的大部分操作都是针对张量进行的。如果数据是其他格式（如 NumPy 数组、Pandas DataFrame 等），需要将其转换为 PyTorch 张量。<br><br>\n",
    "> - 数据各类预处理：在加载数据之前需要进行归一化或其他基础的预处理操作，以确保数据适合训练，可以在 `__getitem__` 方法中添加这些预处理操作。\n",
    "\n",
    "2. **经过DataLoader对数据完成进一步的处理**，包括但不限于：\n",
    "> - 完成batch分割，将数据集转变为特定神经网络能够接纳的格式<br><br>\n",
    "> - 完成padding、裁剪、类型转换等奖数据变得更整齐的预处理操作"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6e06d7c4-112e-487e-a054-1d2973b99e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "de9e007b-91c7-4476-8d78-b3f50246e14f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#兼容Pytorch，我们使用继承自Dataset的类\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 初始化数据集，将传入的数据保存在实例变量data中\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的大小\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 根据索引i获取数据集中的第i个样本\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "9eb796d0-8cc3-4cbf-aac2-c7f470de2174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义collate_fn函数，用于在DataLoader中对一个batch的数据进行处理\n",
    "def collate_fn(examples):\n",
    "    # 将每个样本的输入部分转换为张量\n",
    "    seq = [torch.tensor(ex) for ex in examples]\n",
    "    y_true = [torch.tensor(ex[1:] + [0]) for ex in examples]\n",
    "    \n",
    "    # pytorch自带的padding工具\n",
    "    # 对batch内的样本进行padding，使其具有相同长度\n",
    "    seq = pad_sequence(seq, batch_first=True)\n",
    "    y_true = pad_sequence(y_true, batch_first=True)\n",
    "    \n",
    "    # 返回处理后的输入和目标\n",
    "    return seq, y_true"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058b8d68-57c5-44f1-bcbe-84f2d19b4e48",
   "metadata": {},
   "source": [
    "> - **灵魂拷问：Decoder-Only架构所需的数据输入是什么样子？**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82bc8a0e-4ac8-4a77-9925-f5608ee1d82c",
   "metadata": {},
   "source": [
    "<center><img src=\"https://skojiangdoc.oss-cn-beijing.aliyuncs.com/2023DL/transformer/image-1.png\" alt=\"描述文字\" width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f679e0a8-6fea-4813-9965-484015c6d6c6",
   "metadata": {},
   "source": [
    "几个关键事实：\n",
    "\n",
    "1) 与完整的Transformer不同，Decoder-only结构的输入没有memory，只有标签outputs（在pytorch的代码中一般写作tgt，但极易与损失函数使用的标签所混淆）\n",
    "\n",
    "2) Decoder-Only结构训练的时候是teacher forcing，测试的时候是autoregressive，因此不分特征标签、不分训练集测试集，只有一个序列seq。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "980c1471-eaff-4dad-8244-ccdda1ffccd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "之前讲解过多次，Decoder-Only的结构中预测模式是：\n",
    "\n",
    "训练——（teacher forcing - 不会累计错误）\n",
    "\n",
    "这 👉 xxx1\n",
    "\n",
    "这是 👉 xxx2\n",
    "\n",
    "这是最 👉 xxx3\n",
    "\n",
    "这是最好 👉 xxx4\n",
    "\n",
    "测试——（autoregressive - 累计错误）\n",
    "\n",
    "这是最坏的时代 👉 xxx1\n",
    "\n",
    "这是最坏的时代，xxx1 👉 xxx2\n",
    "\n",
    "这是最坏的时代，xxx1 xxx2 👉 xxx3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35cd43cd-6ded-4a28-95fa-e1e71b95c09e",
   "metadata": {},
   "source": [
    "那seq是什么结构呢？**在embedding之前，是（batch_size，seq_len），就像现在我们得到的ordinal_token一样**。在embedding之后，是（batch_size，seq_len，input_dimension），就是transformer所需要的输入数据格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "db09c59c-1b5c-456b-bf70-8ecd12af8d60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 16, 17, 18, 19, 20, 21, 22, 23, 24, 4, 25, 26, 27, 28, 6, 7, 10, 29, 30, 8, 31, 32, 33, 34, 35, 36, 37, 31, 38, 39, 24, 8, 40, 41, 42, 43, 44, 45, 24, 46, 47, 48, 49, 50, 51, 52, 53, 35, 54, 24, 55, 56, 57, 58, 59, 60, 61, 62, 33, 63, 8, 64, 54, 24, 65, 66, 67, 68, 61, 24, 69, 70, 51, 71, 10, 72, 73, 74, 24, 75, 48, 76, 18, 77, 20, 78, 60, 79, 57, 24, 80, 81, 82, 83, 24, 84, 85, 86, 87, 88, 89, 31, 8, 90, 48, 91, 92, 31, 32, 51, 93, 33, 94, 46, 95, 96, 31, 10, 97, 98, 48, 91, 32, 99, 100, 24, 96, 67, 101, 31, 47, 24, 102, 8, 103, 104, 105, 106, 107, 108, 6, 109, 110, 11, 108, 6, 109, 111, 112, 113, 110, 11, 108, 114, 109, 110, 115, 3, 116, 117, 118, 119, 11, 120, 121, 27, 122, 123, 124, 125, 126, 4, 16, 127, 128, 17, 18, 129, 20, 24, 130, 131, 132, 133, 24, 48, 134, 35, 135, 31, 136, 39, 24, 137, 138, 139, 140, 24, 141, 120, 142, 97, 143, 35, 121, 33, 140, 144, 145, 146, 147, 148, 149, 24, 150, 137, 151, 152, 153, 154, 24, 155, 122, 24, 125, 156, 138, 139, 35, 33, 157, 158, 159, 31, 137, 48, 160, 161, 162, 163, 31, 32, 51, 24, 164, 165, 166, 24, 125, 48, 167, 168, 169, 139, 35, 170, 33, 48, 171, 31, 172, 173, 160, 24, 174, 121, 35, 137, 167, 139, 31, 133, 24, 175, 149, 150, 176, 177, 178, 179, 180, 181, 182, 48, 32, 183, 139, 31, 33, 157, 174, 31, 184, 24, 185, 186, 35, 137, 139, 140, 31, 187, 24, 188, 125, 48, 189, 190, 191, 35, 122, 31, 192, 24, 193, 177, 194, 68, 195, 196, 197, 24, 125, 48, 198, 199, 200, 201, 202, 150, 178, 164, 169, 203, 192, 24, 178, 85, 145, 204, 205, 206, 35, 24, 125, 97, 207, 35, 33, 157, 146, 48, 198, 148, 199, 200, 24, 208, 209, 35, 210, 211, 31, 212, 24, 150, 178, 210, 213, 68, 137, 214, 35, 214, 215, 125, 167, 139, 68, 197, 203, 192, 31, 216, 24, 178, 155, 125, 24, 217, 122, 31, 123, 24, 218, 98, 124, 125, 139, 33, 219, 125, 220, 164, 85, 178, 221, 222, 31, 223, 33, 157, 150, 80, 24, 224, 125, 151, 225, 48, 153, 226, 51, 31, 200, 24, 125, 149, 125, 156, 227, 228, 139, 35, 33, 157, 146, 149, 33, 229, 127, 141, 230, 137, 24, 231, 232, 233, 234, 235, 31, 236, 33, 237, 31, 135, 24, 137, 238, 27, 127, 204, 35, 95, 239, 32, 24, 240, 77, 241, 242, 243, 244, 245, 24, 246, 247, 248, 24, 176, 68, 249, 31, 250, 251, 252, 253, 254, 24, 255, 256, 257, 33, 141, 258, 259, 31, 138] \n",
      " \n",
      " [24, 260, 147, 233, 231, 181, 261, 33, 104, 262, 107, 115, 3, 263, 264, 265, 202, 266, 267, 11, 125, 124, 268, 269, 231, 270, 35, 4, 16, 127, 128, 43, 18, 271, 20, 104, 272, 273, 128, 43, 18, 274, 20, 107, 133, 24, 275, 276, 277, 278, 279, 280, 24, 276, 281, 35, 282, 283, 24, 284, 285, 286, 111, 287, 53, 35, 288, 33, 171, 24, 289, 290, 198, 35, 199, 24, 291, 292, 293, 294, 295, 111, 296, 68, 297, 111, 265, 24, 201, 298, 299, 300, 33, 68, 301, 31, 302, 303, 270, 24, 210, 263, 304, 35, 305, 31, 306, 24, 147, 307, 275, 308, 309, 310, 305, 311, 149, 24, 150, 312, 313, 314, 315, 32, 24, 312, 155, 178, 104, 316, 31, 107, 193, 317, 33, 157, 318, 210, 31, 32, 24, 289, 290, 319, 320, 24, 150, 218, 237, 321, 24, 178, 155, 47, 24, 322, 323, 24, 218, 181, 324, 325, 32, 31, 326, 24, 218, 327, 328, 329, 330, 331, 32, 24, 210, 332, 231, 333, 334, 24, 295, 210, 40, 35, 335, 333, 336, 31, 32, 24, 337, 333, 336, 24, 48, 338, 134, 35, 339, 335, 32, 340, 24, 178, 27, 47, 329, 31, 341, 342, 33, 157, 343, 265, 329, 31, 341, 24, 263, 344, 345, 24, 150, 104, 125, 31, 341, 107, 267, 33, 346, 218, 327, 125, 143, 31, 33, 125, 210, 347, 35, 348, 248, 24, 349, 348, 350, 24, 48, 351, 352, 24, 224, 125, 353, 354, 355, 31, 200, 24, 124, 218, 122, 268, 269, 231, 270, 35, 33, 157, 165, 48, 43, 18, 356, 357, 51, 325, 335, 358, 24, 263, 147, 329, 31, 32, 333, 359, 24, 59, 292, 35, 360, 361, 362, 24, 150, 176, 363, 218, 364, 31, 365, 24, 47, 333, 334, 24, 47, 366, 367, 24, 218, 368, 164, 369, 370, 143, 31, 371, 325, 372, 31, 32, 24, 218, 164, 347, 248, 24, 218, 164, 373, 336, 374, 33, 178, 232, 375, 376, 68, 329, 31, 32, 24, 377, 378, 379, 24, 380, 270, 24, 324, 349, 350, 33, 157, 263, 201, 24, 381, 48, 277, 382, 383, 355, 24, 339, 384, 325, 358, 24, 150, 178, 98, 385, 218, 386, 165, 331, 32, 24, 387, 122, 24, 388, 382, 389, 111, 76, 245, 390, 24, 48, 277, 384, 391, 31, 24, 224, 218, 392, 32, 51, 24, 218, 393, 24, 218, 327, 394, 393, 24, 218, 194, 395, 396, 397, 31, 398, 33, 157, 263, 201, 24, 399, 400, 31, 401, 318, 402, 31, 276, 333, 403, 24, 150, 265, 177, 333, 403, 31, 404, 24, 125, 177, 122, 405, 31, 365, 24, 125, 406, 407, 408, 24, 125, 203, 409, 86, 370, 410, 143, 24, 178, 406, 98, 180, 125, 165, 204, 411, 355, 412, 24, 125, 412, 32, 31, 200, 165, 181, 348, 353, 33, 232, 125, 165, 413, 210, 414, 143, 31, 415, 147, 416, 31, 32, 51, 33, 157, 305, 417, 231, 333, 418, 298, 419, 420, 24, 263, 201] \n",
      " \n",
      " [203, 421, 24, 150, 178, 203, 422, 24, 125, 406, 423, 24, 424, 425, 125, 426, 427, 428, 24, 178, 68, 429, 104, 430, 107, 431, 24, 80, 73, 432, 433, 133, 33, 157, 104, 434, 107, 115, 3, 435, 202, 436, 437, 438, 358, 439, 440, 11, 441, 177, 442, 443, 444, 329, 31, 32, 24, 177, 435, 445, 446, 39, 179, 285, 288, 31, 447, 68, 448, 449, 33, 48, 450, 91, 32, 386, 451, 270, 31, 452, 453, 24, 454, 363, 455, 31, 435, 456, 48, 448, 31, 457, 36, 255, 85, 125, 458, 35, 459, 460, 31, 461, 24, 55, 402, 462, 463, 464, 465, 466, 175, 467, 468, 469, 31, 125, 470, 35, 194, 471, 122, 53, 448, 31, 472, 24, 150, 448, 473, 36, 341, 474, 24, 160, 335, 475, 35, 476, 24, 94, 218, 53, 205, 476, 24, 477, 178, 98, 478, 31, 358, 322, 439, 218, 33, 157, 48, 448, 136, 183, 24, 435, 177, 122, 31, 479, 24, 48, 125, 31, 471, 480, 24, 448, 481, 482, 483, 484, 485, 31, 486, 122, 487, 488, 489, 70, 24, 94, 125, 490, 97, 40, 35, 491, 31, 492, 33, 329, 493, 473, 86, 494, 495, 31, 288, 24, 435, 496, 497, 498, 194, 499, 24, 481, 500, 501, 502, 503, 24, 150, 441, 177, 178, 504, 205, 31, 442, 443, 31, 145, 505, 24, 448, 31, 253, 97, 231, 366, 181, 506, 24, 507, 218, 382, 24, 508, 509, 510, 35, 33, 157, 435, 201, 33, 481, 368, 341, 511, 512, 513, 514, 340, 515, 516, 517, 518, 24, 426, 419, 519, 24, 520, 521, 231, 522, 517, 523, 24, 435, 31, 524, 97, 525, 35, 526, 31, 527, 24, 528, 403, 31, 177, 125, 419, 529, 165, 530, 86, 48, 122, 51, 31, 531, 24, 150, 178, 532, 35, 482, 105, 122, 24, 68, 348, 182, 488, 105, 497, 24, 178, 98, 181, 221, 533, 31, 534, 333, 535, 24, 176, 97, 177, 178, 536, 537, 32, 341, 538, 31, 193, 24, 539, 540, 253, 348, 24, 178, 541, 542, 543, 322, 544, 47, 528, 355, 31, 545, 24, 419, 122, 31, 546, 529, 336, 24, 178, 181, 547, 548, 90, 549, 33, 157, 160, 335, 550, 551, 95, 552, 95, 51, 24, 124, 218, 180, 35, 435, 181, 553, 554, 555, 24, 141, 556, 557, 232, 125, 558, 559, 33, 560, 561, 562, 31, 435, 563, 97, 564, 35, 24, 565, 566, 567, 94, 568, 31, 557, 304, 35, 306, 202, 150, 312, 569, 554, 35, 24, 570, 218, 24, 312, 411, 571, 572, 33, 157, 55, 333, 124, 365, 573, 31, 177, 24, 435, 467, 574, 24, 575, 58, 576, 577, 24, 324, 537, 32, 48, 578, 579, 160, 143, 411, 538, 24, 124, 122, 580, 451, 581, 33, 582, 509, 583, 31, 125, 419, 528, 203, 584, 585, 35, 24, 586, 587, 48, 588, 589, 590, 35, 591, 24, 575, 164, 592, 221, 222, 31, 593, 24, 125, 406, 149, 202, 150, 178, 68, 435, 594, 31, 546, 72, 165]\n"
     ]
    }
   ],
   "source": [
    "print(ordinal_token[0],\"\\n \\n\",ordinal_token[1],\"\\n \\n\",ordinal_token[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd258a-87eb-441b-9d95-fc64d57ac32d",
   "metadata": {},
   "source": [
    "3) dataloader需要整理的不止是Decoder-only结构本身需要的数据，而是整个训练过程中必备的数据。因此除了准备架构的输入之外，还要考虑在计算损失过程中必须的真实标签y_true。那真实标签y_true是什么结构呢？"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d763ced6-0874-447c-855c-cac38ba073ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "依据现在的数据结构，我们的序列长度seq_len为512，因此Decoder-Only架构的训练模式是：\n",
    "\n",
    "这 👉 xxx1\n",
    "[0]    [1]\n",
    "\n",
    "这是 👉 xxx2\n",
    "[0,1]    [2]\n",
    "\n",
    "这是最 👉 xxx3\n",
    "[0,1,2]    [3]\n",
    "\n",
    "这是最好 👉 xxx4\n",
    "[0,1,2,3]    [4]\n",
    "\n",
    "以此类推……"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e721ae9c-4d57-4644-b7b8-244f5ecba6da",
   "metadata": {},
   "source": [
    "因此在Decoder的训练过程中，我们是依据“逐渐变长的句子前半段”来预测“句子的下一个字”。奇妙的是，虽然大部分人会认为上面的过程是按顺序逐渐发生的，但在Decoder架构中上述所有预测是同步进行的，且在Transformer的计算流程中，**上述的每一次预测都是一个样本**："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c663656-6eeb-4394-9888-8ded46c37fb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "样本1:\n",
    "这 👉 xxx1\n",
    "[0]    [1]\n",
    "\n",
    "样本2:\n",
    "这是 👉 xxx2\n",
    "[0,1]    [2]\n",
    "\n",
    "样本3:\n",
    "这是最 👉 xxx3\n",
    "[0,1,2]    [3]\n",
    "\n",
    "样本4:\n",
    "这是最好 👉 xxx4\n",
    "[0,1,2,3]    [4]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6176ed53-3462-4713-8b6e-cf92bbb136be",
   "metadata": {},
   "source": [
    "输入数据是——\n",
    "\n",
    "<table>\n",
    "<tr>\n",
    "  <th>索引</th><th></th><th>y1</th><th>y2</th><th>y3</th><th>y4</th><th>y5</th>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>0</td><td>\"sos\"</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>1</td><td>这</td><td>0.1821</td><td>0.4000</td><td>0.2248</td><td>0.4440</td><td>0.7771</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>2</td><td>是</td><td>0.1721</td><td>0.5030</td><td>0.8948</td><td>0.2385</td><td>0.0987</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>3</td><td>最好的</td><td>0.1342</td><td>0.8297</td><td>0.2978</td><td>0.7120</td><td>0.2565</td>\n",
    "</tr>\n",
    "<tr>\n",
    "  <td>4</td><td>时代</td><td>0.1248</td><td>0.5003</td><td>0.7559</td><td>0.4804</td><td>0.2593</td>\n",
    "</tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c88fdd-92ae-4b1a-82cf-d99f2358d283",
   "metadata": {},
   "source": [
    "然而，从Decoder的掩码注意力层中输出的是**经过掩码后**、每一行只携带特定时间段信息的结果$C_{decoder}$：\n",
    "\n",
    "$$\n",
    "C_{decoder} = \\begin{bmatrix}\n",
    "c_{0} & c_{0} & \\ldots & c_{0} \\\\\n",
    "c_{0 \\to 1} & c_{0 \\to 1} & \\ldots & c_{0 \\to 1} \\\\\n",
    "c_{0 \\to 1} & c_{0 \\to 2} & \\ldots & c_{0 \\to 2} \\\\\n",
    "c_{0 \\to 3} & c_{0 \\to 3} & \\ldots & c_{0 \\to 3} \\\\\n",
    "c_{0 \\to 4} & c_{0 \\to 4} & \\ldots & c_{0 \\to 4} \\\\\n",
    "&……\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "<font color=\"red\">**这里出于教学目的，省略了特征维度上的脚标。现在你所看到的脚标只代表时间维度/序列长度的维度。**\n",
    "\n",
    "从注意力机制中输出的每行数据、就代表了一个“逐渐变长的句子前半段”，因此每行数据也就对应着一个标签。故而在下面的流程中，真实标签就是[1,2,3,4]这样的序列，也就是原始序列seq[1:] + [0]。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8955c17f-d487-4827-bb58-bf7b9627ba93",
   "metadata": {},
   "outputs": [],
   "source": [
    "#兼容Pytorch，我们使用继承自Dataset的类\n",
    "class TransformerDataset(Dataset):\n",
    "    def __init__(self, data):\n",
    "        # 初始化数据集，将传入的数据保存在实例变量data中\n",
    "        self.data = data\n",
    "\n",
    "    def __len__(self):\n",
    "        # 返回数据集的大小\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        # 根据索引i获取数据集中的第i个样本\n",
    "        return self.data[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "13ac6afe-5128-4d71-b53c-9ebe5680d90c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义collate_fn函数，用于在DataLoader中对一个batch的数据进行处理\n",
    "def collate_fn(examples):\n",
    "    # 将每个样本的输入部分转换为张量\n",
    "    seq = [torch.tensor(ex) for ex in examples]\n",
    "    y_true = [torch.tensor(ex[1:] + [0]) for ex in examples]\n",
    "    \n",
    "    # pytorch自带的padding工具\n",
    "    # 对batch内的样本进行padding，使其具有相同长度\n",
    "    seq = pad_sequence(seq, batch_first=True)\n",
    "    y_true = pad_sequence(y_true, batch_first=True)\n",
    "    \n",
    "    # 返回处理后的输入和目标\n",
    "    return seq, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9f47a067-93b9-45ba-8557-3ff49368e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "dataset = TransformerDataset(ordinal_token)  # 创建数据集\n",
    "dataloader = DataLoader(dataset\n",
    "                        , batch_size=batch_size\n",
    "                        , drop_last = False\n",
    "                        , collate_fn=collate_fn\n",
    "                        , shuffle=False)  # 创建训练数据加载器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d582c698-e513-4490-8264-ec7a05967fd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 4, 16, 17, 18, 19, 20, 21, 22, 23, 24, 4, 25, 26, 27, 28, 6, 7, 10, 29, 30, 8, 31, 32, 33, 34, 35, 36, 37, 31, 38, 39, 24, 8, 40, 41, 42, 43, 44, 45, 24, 46, 47, 48, 49, 50, 51, 52, 53, 35, 54, 24, 55, 56, 57, 58, 59, 60, 61, 62, 33, 63, 8, 64, 54, 24, 65, 66, 67, 68, 61, 24, 69, 70, 51, 71, 10, 72, 73, 74, 24, 75, 48, 76, 18, 77, 20, 78]\n"
     ]
    }
   ],
   "source": [
    "for x in dataset:\n",
    "    print(x[:100])\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f0dab85b-f397-456a-b57c-9bd3d37ca3f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[   3,    4,    5,  ...,  259,   31,  138],\n",
      "        [  24,  260,  147,  ...,   24,  263,  201],\n",
      "        [ 203,  421,   24,  ...,  546,   72,  165],\n",
      "        ...,\n",
      "        [  24, 3256,   31,  ...,  366, 3349,   24],\n",
      "        [3350, 1142,  218,  ..., 3262,  546,  419],\n",
      "        [  97,  177, 3409,  ..., 3215,  479,  600]]), tensor([[   4,    5,    6,  ...,   31,  138,    0],\n",
      "        [ 260,  147,  233,  ...,  263,  201,    0],\n",
      "        [ 421,   24,  150,  ...,   72,  165,    0],\n",
      "        ...,\n",
      "        [3256,   31,  445,  ..., 3349,   24,    0],\n",
      "        [1142,  218, 3268,  ...,  546,  419,    0],\n",
      "        [ 177, 3409,   33,  ...,  479,  600,    0]])) \n",
      "\n",
      "\n",
      "seq: \n",
      " tensor([[   3,    4,    5,  ...,  259,   31,  138],\n",
      "        [  24,  260,  147,  ...,   24,  263,  201],\n",
      "        [ 203,  421,   24,  ...,  546,   72,  165],\n",
      "        ...,\n",
      "        [  24, 3256,   31,  ...,  366, 3349,   24],\n",
      "        [3350, 1142,  218,  ..., 3262,  546,  419],\n",
      "        [  97,  177, 3409,  ..., 3215,  479,  600]]) \n",
      "\n",
      "\n",
      "torch.Size([32, 512]) \n",
      "\n",
      "\n",
      "y_true: \n",
      " tensor([[   4,    5,    6,  ...,   31,  138,    0],\n",
      "        [ 260,  147,  233,  ...,  263,  201,    0],\n",
      "        [ 421,   24,  150,  ...,   72,  165,    0],\n",
      "        ...,\n",
      "        [3256,   31,  445,  ..., 3349,   24,    0],\n",
      "        [1142,  218, 3268,  ...,  546,  419,    0],\n",
      "        [ 177, 3409,   33,  ...,  479,  600,    0]]) \n",
      "\n",
      "\n",
      "torch.Size([32, 512])\n"
     ]
    }
   ],
   "source": [
    "for batch in dataloader:\n",
    "    print(batch,\"\\n\\n\") # 第一个batch中的实际数据，包括需要输入的seq与标签y_true\n",
    "    print(\"seq: \\n\", batch[0],\"\\n\\n\") #seq，结构为(batch_size, seq_len)\n",
    "    print(batch[0].shape,\"\\n\\n\")\n",
    "    print(\"y_true: \\n\", batch[1],\"\\n\\n\") #y_true，结构也为(batch_size,seq_len)\n",
    "    print(batch[1].shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cfc5464-369e-4c83-94d9-02d892001a7e",
   "metadata": {},
   "source": [
    "对生成式算法而言，我们不分训练集与测试集，同时我们也不分特征和标签，因为我们是用句子的前半段去预测句子的后半段。在生成式任务中，比如文本生成或语言模型训练，常常采用自回归模型（autoregressive model）来逐步预测序列中的下一个元素。这种情况下，通常不需要明确地划分训练集和测试集，而是通过给定的序列前半部分来预测后半部分，从而进行训练和评估。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
